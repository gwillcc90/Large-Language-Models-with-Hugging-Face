{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04450e3a-e925-498e-9d1c-5bd568d5130c",
   "metadata": {},
   "source": [
    "#### 1. What's the order of the Language Modeling Pipeline?\n",
    "\n",
    "The tokenizer splits the text up and encodes the pieces (converts to numbers), then the numeric tokens are passed to the model, which outputs a prediction tensor that depends on the task, then the tokenizer can be used to convert that output tensor back to tokens (if applicable).\n",
    "\n",
    "#### 2. How many dimensions are does the tensor of the output of the base Tranformer model have? What are they?\n",
    "\n",
    "The output tensor of the base Tranformer model has 3 dimensions:\n",
    "1. Batch: how many sequences were input into the model\n",
    "2. Length: length of the maximum-length sequence\n",
    "3. Hidden-shape: the input size of the model (I think...)\n",
    "\n",
    "#### 3. Which of the following are subword tokenization algorithms or part of one?\n",
    "\n",
    "**WordPiece**, splitting on whitespace, splitting on characters, **BPE**, **Unigram**, None of the above.\n",
    "\n",
    "#### 4. What is a model head?\n",
    "\n",
    "A model head, or **Adaptation head**, is what transforms predictions for their related task.\n",
    "\n",
    "#### 5. What is a TFAutoModel?\n",
    "\n",
    "A general class for getting a model architecture based on its checkpoint.\n",
    "\n",
    "#### 6. What are some techniques to use when batching sequences of different lengths?\n",
    "\n",
    "Truncating, padding, attention masking\n",
    "\n",
    "#### 7. Why is softmax applied to logits?\n",
    "\n",
    "To convert the logits to probabilities.\n",
    "\n",
    "#### 8. What method is the Tokenizer API centered around?\n",
    "\n",
    "Kinda a trick question... it's centered around the ```__call__``` method.\n",
    "\n",
    "#### 9. What does the code do:\n",
    "\n",
    "```\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "tokenizer.tokenize('Hello!')\n",
    "```\n",
    "\n",
    "The code splits the given sequence into tokens. Nothing else.\n",
    "\n",
    "#### 10. Is there something wrong with the code:\n",
    "\n",
    "```\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "model = TFAutoModel.from_pretrained('gpt2')\n",
    "\n",
    "encoded = tokenizer('Hey!', return_tensors = 'pt')\n",
    "result = model(**encoded)\n",
    "```\n",
    "\n",
    "The tokenizer and model are different. Recall that the tokenizer and model should be initialized from the same checkpoint due to the need of model input to be representative of the vocabulary it was trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c84af5-1247-47bf-90e7-c952d7cdbdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
