{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2c914ce-01df-482a-98d1-12910f56c6cf",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "\n",
    "## NLP Overview\n",
    "\n",
    "The intersection of Machine Learning and Linguistics is NLP.\n",
    "\n",
    "The underlying task in NLP is to understand what words mean in the context of their sentences.\n",
    "\n",
    "Some problems NLP aims to solve:\n",
    "- Classifying sentences\n",
    "  - Ham/spam emails, real/fake news, sentiment analysis\n",
    "- Classifying words\n",
    "  - Parts of speech\n",
    "- Text Generation\n",
    "  - Prompt completion\n",
    "  - Answer extraction\n",
    "  - Text summarization\n",
    "  - Translation\n",
    " \n",
    "To stress \"what words mean in the context of their sentences\", take a moment to think about how each NLP problem above relies context.\n",
    "\n",
    "NLP also deals with Computer Vision and Speech Recognition for tasks like generating a description of an image and generating a text-transcript from an audio sample.\n",
    "\n",
    "## Hugging Face Transformers Library\n",
    "\n",
    "The transformers library allows us to easily perform NLP tasks with the use of the `pipeline` class.\n",
    "\n",
    "### Pipelines\n",
    "\n",
    "As a first introduction to the `pipeline` object in the `transformers` library, know that *each task NLP task* for `pipeline` modes (like \"text-generation\", \"summarization\", \"translation\", etc) *includes a default model* that has been fine-tuned for its task. See below that there are no other arguments passed with the task-type to the `pipeline` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4a7c720-e46a-417f-ba29-2e085ea8e68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\linda\\OneDrive\\Desktop\\Large Language Models\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\linda\\OneDrive\\Desktop\\Large Language Models\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab62f6ba-b773-4ab3-8481-b2cb5663a303",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998635053634644}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "pipeline has no other arguments other than the task-type\n",
    "and downloads a default model for the task.\n",
    "'''\n",
    "classifier = pipeline('sentiment-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77e892a-8b92-458c-bb18-7d8432bee5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier('you look really nice today!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "931580c5-4acb-4eb6-bad1-8dd0b1ecd8bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998635053634644},\n",
       " {'label': 'NEGATIVE', 'score': 0.9995715022087097}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = ['you look really nice today!', 'you look really terrible today!']\n",
    "classifier(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d456285-dee2-40ff-b8e8-4cab037a1fe1",
   "metadata": {},
   "source": [
    "**We can also specify models from the Hugging Face website by using part of the path to its page (on Hugging Face).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a7ac0516-d0ca-4e59-a284-238f6f65a53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier = pipeline('text-generation',model=\"distilgpt2\",\n",
    "#                      trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22555a81-ee97-4a40-b44e-36dbc31291b0",
   "metadata": {},
   "source": [
    "We can search for models here https://huggingface.co/models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cf331d-1677-4edd-b0b6-5c28446964e6",
   "metadata": {},
   "source": [
    "Hugging Face Pipelines are comprehensive solutions to performing the back-end of an NLP task.\n",
    "\n",
    "Pipeline jobs:\n",
    "1. Preprocess input\n",
    "2. pass the input to the model (which has been downloaded and cached by the `pipeline` object)\n",
    "3. post-process input for human readability\n",
    "\n",
    "Effectively, Hugging Face has done all the work for us, we just need to tell the `pipeline` which task we are performing, among other parameters which we won't cover in this notebook.\n",
    "\n",
    "Below, we will test out the `pipeline`s for tasks listed on the Hugging Face tutorial:\n",
    "- Zero-shot classification\n",
    "- Text generation\n",
    "- mask filling\n",
    "- named entity recognition\n",
    "- question answering\n",
    "- summarization\n",
    "- translation\n",
    "\n",
    "But this is not a comprehensive list of Hugging Face Pipeline options.\n",
    "\n",
    "See https://huggingface.co/docs/transformers/main_classes/pipelines for a comprehensive list of `pipelines`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8966474-a375-4de1-96d7-22ad059248b6",
   "metadata": {},
   "source": [
    "#### Zero-shot classification\n",
    "\n",
    "The \"zero-shot-classification\" pipeline allows us to classify text-input by giving a set of possible categories. It is called zero-shot because there is no need to fine-tune the model.\n",
    "\n",
    "It outputs the probability that the text belongs to each output, and adds up to 100%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "536bc872-e61e-4bd4-8c2d-b17e69f61c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to roberta-large-mnli and revision 130fb28 (https://huggingface.co/roberta-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline('zero-shot-classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6d63722-ca7b-471b-b625-33c0a126edf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'This product smells really nice',\n",
       " 'labels': ['aesthetics', 'academics', 'sports'],\n",
       " 'scores': [0.9762329459190369, 0.01219076570123434, 0.011576291173696518]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier('This product smells really nice',\n",
    "         candidate_labels = ['aesthetics', 'sports', 'academics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1e67493-90d5-45ae-9a6a-b55c01eb0529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'This product smells really nice',\n",
       " 'labels': ['aesthetics', 'food', 'academics'],\n",
       " 'scores': [0.9493024349212646, 0.038843072950839996, 0.011854469776153564]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier('This product smells really nice',\n",
    "         candidate_labels = ['aesthetics', 'food', 'academics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "150ef5c3-387a-4c70-b3c1-378d0efbe548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'This product smells really nice',\n",
       " 'labels': ['aesthetics', 'home goods', 'food'],\n",
       " 'scores': [0.9077967405319214, 0.05505849048495293, 0.03714476525783539]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier('This product smells really nice',\n",
    "         candidate_labels = ['aesthetics', 'food', 'home goods'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc45cbc-c255-4526-9ce4-b51fae3aab2d",
   "metadata": {},
   "source": [
    "#### Text Generation\n",
    "\n",
    "Like GPT, the \"text-generation\" pipeline will complete a prompt.\n",
    "\n",
    "It's worth noting that the given, default model performs nowhere near similarly to GPT3.5; GPT3.5 is much, much better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8749d559-7211-48eb-9269-694152c8a574",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to gpt2 and revision 6c0e608 (https://huggingface.co/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline('text-generation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac5ef036-54ed-46af-9e79-804c9e677a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"System: You are an assistant that tells what day it is.\\nUser: what day is today?\\nUser: the day before?\\nUser: is tomorrow night?\\nUser: no day tomorrow\\nUser: it's Monday tomorrow\\nUser\"}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = generator('System: You are an assistant that tells what day it is.\\nUser: what day is today?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "778e5994-1008-4e31-9281-713cf04956f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are an assistant that tells what day it is.\n",
      "User: what day is today?\n",
      "User: the day before?\n",
      "User: is tomorrow night?\n",
      "User: no day tomorrow\n",
      "User: it's Monday tomorrow\n",
      "User\n"
     ]
    }
   ],
   "source": [
    "print(a[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57ef67b1-31f8-4072-bf8e-496763602376",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "a = generator('What is the numeric date of the day after Christmas day?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c7e3e592-bcdd-49a0-8219-d6123dc52720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the numeric date of the day after Christmas day?\n",
      "\n",
      "An accurate date, but a date in the format \"F0-6\"; you can see the exact date in hexadecimal format on the wiki.\n",
      "\n",
      "Is this\n"
     ]
    }
   ],
   "source": [
    "print(a[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "63ad3baf-2a03-48f0-bb93-17bde471d62e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'In this course, we will teach you how to run a web application using Python and Ruby.\\n\\nPlease read in more detail how to apply it, in short, to your web application. Please note the following:\\n\\nMake sure you understand'}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"In this course, we will teach you how to\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0f114212-50c3-4029-8b34-4418beb6fc19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'In this course, we will teach you how to build an authentic relationship with the person you have with the world, and how to set a new foundation for your work day. After that, we will talk practical techniques and examples of how to work with'}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"In this course, we will teach you how to\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "84f7463c-ea10-4f25-b309-e0152b1b3661",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'In this course, we will teach you how to use a Raspberry Pi 3 as a portable desktop computer. Please use it to'},\n",
       " {'generated_text': 'In this course, we will teach you how to use an iPhone with a camera on a USB and it will work on any'},\n",
       " {'generated_text': 'In this course, we will teach you how to use a JavaScript class to manage two sets of users in the context of a'},\n",
       " {'generated_text': 'In this course, we will teach you how to use the new Ruby on Rails and how to build your own custom Ruby application'},\n",
       " {'generated_text': \"In this course, we will teach you how to design a functional JavaScript app. We'll cover the fundamentals and see how to\"}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"In this course, we will teach you how to\",\n",
    "          num_return_sequences = 5,\n",
    "          max_length = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70acda0a-8d57-4463-8c13-b64dd9edecfa",
   "metadata": {},
   "source": [
    "#### Mask Filling\n",
    "\n",
    "Filling in the blanks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "03b002a8-ab03-479d-b4b0-b38126b1ca04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilroberta-base and revision ec58a5b (https://huggingface.co/distilroberta-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "config.json: 100%|████████████████████████████████████████████████████████████████████████████| 480/480 [00:00<?, ?B/s]\n",
      "model.safetensors: 100%|████████████████████████████████████████████████████████████| 331M/331M [00:12<00:00, 27.4MB/s]\n",
      "All PyTorch model weights were used when initializing TFRobertaForMaskedLM.\n",
      "\n",
      "All the weights of TFRobertaForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForMaskedLM for predictions without further training.\n",
      "vocab.json: 100%|███████████████████████████████████████████████████████████████████| 899k/899k [00:00<00:00, 6.98MB/s]\n",
      "merges.txt: 100%|███████████████████████████████████████████████████████████████████| 456k/456k [00:00<00:00, 12.2MB/s]\n",
      "tokenizer.json: 100%|█████████████████████████████████████████████████████████████| 1.36M/1.36M [00:00<00:00, 16.6MB/s]\n"
     ]
    }
   ],
   "source": [
    "unmasker = pipeline('fill-mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d34bc4f3-4a80-4641-ba49-5ab5f60a0597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.19890430569648743,\n",
       "  'token': 30412,\n",
       "  'token_str': ' mathematical',\n",
       "  'sequence': 'This course will teach you about mathematical models.'},\n",
       " {'score': 0.05367986112833023,\n",
       "  'token': 38163,\n",
       "  'token_str': ' computational',\n",
       "  'sequence': 'This course will teach you about computational models.'}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker('This course will teach you about <mask> models.', \n",
    "         top_k = 2 # How many values we want to return\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18027a4b-e1db-452d-8197-a2568d22c1be",
   "metadata": {},
   "source": [
    "#### Named-Entity Recognition\n",
    "\n",
    "Finds the entities in the given input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4b53a43f-8837-4bee-aa97-725c131422b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "config.json: 100%|████████████████████████████████████████████████████████████████████████████| 998/998 [00:00<?, ?B/s]\n",
      "model.safetensors: 100%|██████████████████████████████████████████████████████████| 1.33G/1.33G [00:48<00:00, 27.7MB/s]\n",
      "All PyTorch model weights were used when initializing TFBertForTokenClassification.\n",
      "\n",
      "All the weights of TFBertForTokenClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForTokenClassification for predictions without further training.\n",
      "tokenizer_config.json: 100%|████████████████████████████████████████████████████████| 60.0/60.0 [00:00<00:00, 29.4kB/s]\n",
      "vocab.txt: 100%|████████████████████████████████████████████████████████████████████| 213k/213k [00:00<00:00, 3.00MB/s]\n"
     ]
    }
   ],
   "source": [
    "ner = pipeline('ner', grouped_entities = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e00f69d2-6e62-4de3-8285-4be722b2d658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.9791667,\n",
       "  'word': 'Will Curkan',\n",
       "  'start': 11,\n",
       "  'end': 22}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner('My name is Will Curkan and I work for a psychiatric and psychological services company.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "45a747f9-c355-459a-8a4c-1dc7455d28b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.99894196,\n",
       "  'word': 'Bob',\n",
       "  'start': 11,\n",
       "  'end': 14},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.97730756,\n",
       "  'word': 'Meta',\n",
       "  'start': 58,\n",
       "  'end': 62}]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner('My name is Bob and I drive my lamborghini urus to work at Meta.') # Didnt catch lambo!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d69fb6c-8c1b-4f34-bcd4-d1a67c0d0c03",
   "metadata": {},
   "source": [
    "#### Question Answering\n",
    "\n",
    "Given a `question` and some `context`, we can get an answer.\n",
    "\n",
    "**This pipeline DOES NOT GENERATE an answer, it gets it from the context**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1f1fa7b-67a8-4125-b1a2-8c89d4aa3fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "config.json: 100%|█████████████████████████████████████████████████████████████████████| 473/473 [00:00<00:00, 474kB/s]\n",
      "model.safetensors: 100%|████████████████████████████████████████████████████████████| 261M/261M [00:10<00:00, 25.6MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\linda\\OneDrive\\Desktop\\Large Language Models\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForQuestionAnswering.\n",
      "\n",
      "All the weights of TFDistilBertForQuestionAnswering were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForQuestionAnswering for predictions without further training.\n",
      "tokenizer_config.json: 100%|████████████████████████████████████████████████████████████████| 29.0/29.0 [00:00<?, ?B/s]\n",
      "vocab.txt: 100%|████████████████████████████████████████████████████████████████████| 213k/213k [00:00<00:00, 1.90MB/s]\n",
      "tokenizer.json: 100%|███████████████████████████████████████████████████████████████| 436k/436k [00:00<00:00, 10.8MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.5120940208435059,\n",
       " 'start': 0,\n",
       " 'end': 48,\n",
       " 'answer': 'A psychiatric and psychological services company'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_question = pipeline('question-answering')\n",
    "answer_question(question = 'Where do I work?',\n",
    "                context = 'A psychiatric and psychological services company')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57d3c58e-74f0-485d-a329-9744648214ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.9465346932411194, 'start': 41, 'end': 50, 'answer': 'somewhere'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_question(question = 'Where do I work?',\n",
    "                context = 'Not a real context... I do actually work somewhere, as stated above, but this is just an example')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff233548-e7cc-4d6e-98fe-d4888d5374fd",
   "metadata": {},
   "source": [
    "**Note the response completely depends on the context** (also I love the answer \"somewhere\" haha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bc94a0-7897-43e1-b42c-a95999a2c704",
   "metadata": {},
   "source": [
    "#### Summarization\n",
    "\n",
    "In Summarization tasks, the model seeks to find the important parts of the message, then condense it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3614ecc4-23d2-4d51-8459-86d68713f041",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2905e0-b922-409c-a9ef-0bde1ceb295d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45690560-0c45-4265-adee-9c6261456d57",
   "metadata": {},
   "source": [
    "#### Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68147ddd-8987-46aa-be8e-c3830c301581",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ca0721-834a-4b74-99cd-0cab55358a32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
