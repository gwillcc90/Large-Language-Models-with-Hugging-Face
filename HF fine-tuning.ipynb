{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "090922f8-167e-4b78-9b26-78b5230b847c",
   "metadata": {},
   "source": [
    "# Fine Tuning\n",
    "\n",
    "Fine-tuning is to further train a pre-trained model. Continuing to train a pre-trained model is to update its weights (parameters) to work better on your dataset.\n",
    "\n",
    "If you're familiar with using ANNs in TensorFlow, this is as simple as continueing training of the model on the new dataset.\n",
    "\n",
    "I wanted to use the dataset and follow the example on the Hugging Face course, but I could not get any dataset to load. I couldhave used `requests` to work-around this, but I'm going to work on a the Sephora makeup reviews dataset from Kaggle.\n",
    "\n",
    "The `requests` work-around would be something like:\n",
    "```\n",
    "import requests\n",
    "r = requests.get(\"uri_to_dataset\")\n",
    "r.json()\n",
    "```\n",
    "\n",
    "Also, I do not have enough ram to hold the dataset so I utilize file IO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd824106-0550-495c-8804-d5fa13138c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Condense the five files containing reviews into one.\n",
    "'''\n",
    "import os\n",
    "\n",
    "FILENAME = 'reviews_complete.csv'\n",
    "PATH = '.\\\\Sephora Reviews'\n",
    "files = os.listdir(PATH)\n",
    "num_files = range(len(files))\n",
    "\n",
    "if not os.path.exists(os.getcwd() + '\\\\' + FILENAME):\n",
    "    with open(FILENAME, 'wb') as f:\n",
    "        for i, file in zip(num_files, files):\n",
    "            if not i:\n",
    "                with open(PATH + '\\\\' + file, 'rb') as f2:\n",
    "                    f.write(f2.read())\n",
    "            else: # Then who!\n",
    "                with open(PATH + '\\\\' + file, 'rb') as f2:\n",
    "                    f2.readline()\n",
    "                    f.write(f2.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "274495c6-965b-4aae-bbd3-19ca44ce897b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Pull 10000 records at a time.\n",
    "Transform them.\n",
    "Place them in a new file.\n",
    "'''\n",
    "import pandas as pd\n",
    "\n",
    "# Variables/Constants\n",
    "FILENAME = 'sephora_reviews_preprocessed.csv'\n",
    "col_names = pd.read_csv('reviews_complete.csv', nrows = 1).columns\n",
    "skiprows = 1 # Skip first one bc we have headers\n",
    "NROWS = 10000\n",
    "num_rows = 10000\n",
    "\n",
    "# lambda\n",
    "clean_nans = lambda x : '' if x is np.nan else x\n",
    "\n",
    "# prepreprocessing\n",
    "if not os.path.exists(os.getcwd() + '\\\\' + FILENAME):\n",
    "    with open(FILENAME, 'w') as f:\n",
    "        while num_rows >= NROWS:\n",
    "            dataset = pd.read_csv('reviews_complete.csv', skiprows = skiprows, nrows = NROWS, names = col_names)\n",
    "            num_rows = dataset.shape[0]\n",
    "            dataset['full_review'] = \"Review title: \" + dataset.review_title + \"; Review text: \" + dataset.review_text\n",
    "            dataset.full_review = dataset.full_review.apply(clean_nans)\n",
    "\n",
    "            f.writelines(\n",
    "                [ \n",
    "                  ','.join( [str(token) for token in tokens] ) + \"\\n\" \n",
    "                  for tokens in tokenizer(list(dataset.full_review), padding = True,).input_ids \n",
    "                ]\n",
    "            )\n",
    "\n",
    "            skiprows += NROWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "292e17af-55d7-46d0-bd23-3f2ede13388f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 20)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ebe96c60-92a1-41b1-b697-01f7ab67f695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\linda\\OneDrive\\Desktop\\Large Language Models\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\linda\\OneDrive\\Desktop\\Large Language Models\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\linda\\OneDrive\\Desktop\\Large Language Models\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Tokenize the reviews\n",
    "'''\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b6b5b6-5cf3-40df-bf28-4bd0d91e06e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(list(dataset.full_review), padding = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8d8d1b94-01fc-44ff-8f4e-a88918bafbcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0',\n",
       " '32773',\n",
       " '1270',\n",
       " '35',\n",
       " '255',\n",
       " '19906',\n",
       " '162',\n",
       " '141',\n",
       " '7',\n",
       " '1457',\n",
       " '2382',\n",
       " '1090',\n",
       " '328',\n",
       " '131',\n",
       " '5872',\n",
       " '2788',\n",
       " '35',\n",
       " '38',\n",
       " '304',\n",
       " '42',\n",
       " '19',\n",
       " '5',\n",
       " '234',\n",
       " '1906',\n",
       " '990',\n",
       " '3181',\n",
       " '44',\n",
       " '48',\n",
       " '347',\n",
       " '405',\n",
       " '14888',\n",
       " '10326',\n",
       " '4317',\n",
       " '119',\n",
       " '359',\n",
       " '5293',\n",
       " '12',\n",
       " '10926',\n",
       " '30371',\n",
       " '17',\n",
       " '48',\n",
       " '7',\n",
       " '1457',\n",
       " '2382',\n",
       " '1090',\n",
       " '8',\n",
       " '24',\n",
       " '34',\n",
       " '2198',\n",
       " '1714',\n",
       " '127',\n",
       " '3024',\n",
       " '36',\n",
       " '1990',\n",
       " '5',\n",
       " '357',\n",
       " '322',\n",
       " '20',\n",
       " '146',\n",
       " '12',\n",
       " '658',\n",
       " '20147',\n",
       " '16',\n",
       " '681',\n",
       " '716',\n",
       " '8',\n",
       " '24508',\n",
       " '70',\n",
       " '9',\n",
       " '110',\n",
       " '7855',\n",
       " '2422',\n",
       " '2773',\n",
       " '4',\n",
       " '38',\n",
       " '1407',\n",
       " '12',\n",
       " '658',\n",
       " '19',\n",
       " '42',\n",
       " '514',\n",
       " '716',\n",
       " '30317',\n",
       " '254',\n",
       " '6',\n",
       " '8',\n",
       " '38',\n",
       " '67',\n",
       " '304',\n",
       " '42',\n",
       " '95',\n",
       " '30',\n",
       " '1495',\n",
       " '77',\n",
       " '38',\n",
       " '17',\n",
       " '27',\n",
       " '119',\n",
       " '45',\n",
       " '2498',\n",
       " '146',\n",
       " '12',\n",
       " '658',\n",
       " '4',\n",
       " '85',\n",
       " '3607',\n",
       " '5',\n",
       " '3024',\n",
       " '18399',\n",
       " '30317',\n",
       " '196',\n",
       " '6',\n",
       " '53',\n",
       " '396',\n",
       " '28546',\n",
       " '5',\n",
       " '3024',\n",
       " '4',\n",
       " '158',\n",
       " '73',\n",
       " '698',\n",
       " '5940',\n",
       " '15224',\n",
       " '19',\n",
       " '5',\n",
       " '146',\n",
       " '12',\n",
       " '658',\n",
       " '20147',\n",
       " '4',\n",
       " '85',\n",
       " '17',\n",
       " '27',\n",
       " '29',\n",
       " '19858',\n",
       " '328',\n",
       " '2']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(str, tokenizer(list(dataset.full_review)).input_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514995f7-c76b-4bea-b96a-41b64f17b209",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e5675a95-ebc5-4fbd-ab1b-931030b810c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1301136 entries, 0 to 1301135\n",
      "Data columns (total 19 columns):\n",
      " #   Column                    Non-Null Count    Dtype  \n",
      "---  ------                    --------------    -----  \n",
      " 0   Unnamed: 0                1301136 non-null  int64  \n",
      " 1   author_id                 1301136 non-null  object \n",
      " 2   rating                    1301136 non-null  int64  \n",
      " 3   is_recommended            1107162 non-null  float64\n",
      " 4   helpfulness               631670 non-null   float64\n",
      " 5   total_feedback_count      1301136 non-null  int64  \n",
      " 6   total_neg_feedback_count  1301136 non-null  int64  \n",
      " 7   total_pos_feedback_count  1301136 non-null  int64  \n",
      " 8   submission_time           1301136 non-null  object \n",
      " 9   review_text               1299520 non-null  object \n",
      " 10  review_title              930754 non-null   object \n",
      " 11  skin_tone                 1103798 non-null  object \n",
      " 12  eye_color                 1057734 non-null  object \n",
      " 13  skin_type                 1172830 non-null  object \n",
      " 14  hair_color                1037824 non-null  object \n",
      " 15  product_id                1301136 non-null  object \n",
      " 16  product_name              1301136 non-null  object \n",
      " 17  brand_name                1301136 non-null  object \n",
      " 18  price_usd                 1301136 non-null  float64\n",
      "dtypes: float64(3), int64(5), object(11)\n",
      "memory usage: 188.6+ MB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
