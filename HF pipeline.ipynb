{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79aff2a5-fe18-49b2-8b13-ba0e0206544a",
   "metadata": {},
   "source": [
    "# The Hugging Face Pipeline\n",
    "\n",
    "First, why are we even using Hugging Face? Is there something better? What does Hugging Face offer us?\n",
    "\n",
    "Hugging Face strives to the *the* hub for sharing LLMs. Because anyone can share LLMs, and big companies *do* share their LLMs here, we can use models with 10s of billions of parameters with little fuss.\n",
    "\n",
    "Hugging Face advertises:\n",
    "- **It's easy**: \"Downloading, loading, and using LLMs is done in a few lines of code\"\n",
    "  - We see this is true by the by performing a task in two lines of code using `pipeline`\n",
    "- **Flexibility**: Everything is of the class Pytorch and TensorFlow\n",
    "\n",
    "We've used the `pipeline` function.\n",
    "\n",
    "The `pipeline` function is abstracting away the process of:\n",
    "1. **Tokenization**: Preprocessing data; tokenization\n",
    "2. **Model Input**: Inputting the preprocessed data into the model and outputting logits\n",
    "3. **Postprocessing**: converting logits to probabilities, and getting the results\n",
    "\n",
    "We will now discuss each of these, individually.\n",
    "\n",
    "## Tokenization\n",
    "\n",
    "Neural networks cannot process textual data. It must be \"Tokenized\".\n",
    "\n",
    "Plainly, the Tokenization process has three steps:\n",
    "1. Converting textual data (sentences, phrases, words) into words, subwords, or symbols, all being **tokens**.\n",
    "2. Adding special characters that the model might need to separate sentences.\n",
    "3. Converting the tokens into a numeric representations.\n",
    "\n",
    "Each model has it's own preprocessing (tokenization) method. You might ask \"how does this make sense?\" \n",
    "\n",
    "It makes sense because numerically encoding words/tokens and training a model means that if one does not give words the same encoding as they were given in training, then the words effectively aren't the same. Imagine if you read two dictionaries and the definitions of words were completely different!\n",
    "\n",
    "Hugging Face allows us to easily use model's original preprocessing techniques through the `AutoTokenizer` class.\n",
    "\n",
    "*Def'n.* **checkpoint** - The weights used as the parameters for the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5b68464-70ec-4a05-98b8-fd3915ea653a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\linda\\OneDrive\\Desktop\\Large Language Models\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\linda\\OneDrive\\Desktop\\Large Language Models\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "{'input_ids': <tf.Tensor: shape=(2, 12), dtype=int32, numpy=\n",
      "array([[  101, 17662,  2227,  2003,  1037,  3835,  4037,   999,   102,\n",
      "            0,     0,     0],\n",
      "       [  101,  1045,  2293,  2000,  4553,  2055,  6627,  1011,  3141,\n",
      "         7832,   999,   102]])>, 'attention_mask': <tf.Tensor: shape=(2, 12), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
      "{'input_ids': <tf.Tensor: shape=(2, 12), dtype=int32, numpy=\n",
      "array([[  101, 17662,  2227,  2003,  1037,  3835,  4037,   999,   102,\n",
      "            0,     0,     0],\n",
      "       [  101,  1045,  2293,  2000,  4553,  2055,  6627,  1011,  3141,\n",
      "         7832,   999,   102]])>, 'attention_mask': <tf.Tensor: shape=(2, 12), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])>}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "data = ['Hugging Face is a nice website!',\n",
    "        'I love to learn about tech-related topics!']\n",
    "\n",
    "inputs = tokenizer(data, padding = True, truncation = True, return_tensors = 'tf')\n",
    "print(inputs)\n",
    "\n",
    "inputs = tokenizer(data, padding = True,return_tensors = 'tf')\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60ce4fc-9adc-4f9f-9565-1171b42fae5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33404a18-60fa-43da-87a6-d6a49504489f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f0fa80-1062-4426-a1c2-5bae66756ce0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59e0d3c-1961-4a30-a444-1b535be02fb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
