{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79aff2a5-fe18-49b2-8b13-ba0e0206544a",
   "metadata": {},
   "source": [
    "# The Hugging Face Pipeline\n",
    "\n",
    "First, why are we even using Hugging Face? Is there something better? What does Hugging Face offer us?\n",
    "\n",
    "Hugging Face strives to the *the* hub for sharing LLMs. Because anyone can share LLMs, and big companies *do* share their LLMs here, we can use models with 10s of billions of parameters with little fuss.\n",
    "\n",
    "Hugging Face advertises:\n",
    "- **It's easy**: \"Downloading, loading, and using LLMs is done in a few lines of code\"\n",
    "  - We see this is true by the by performing a task in two lines of code using `pipeline`\n",
    "- **Flexibility**: Everything is of the class Pytorch and TensorFlow\n",
    "\n",
    "We've used the `pipeline` function.\n",
    "\n",
    "The `pipeline` function is abstracting away the process of:\n",
    "1. **Tokenization/Encoding**: Preprocessing data; tokenization\n",
    "2. **Model Input**: Inputting the preprocessed data into the model and outputting logits\n",
    "3. **Postprocessing**: converting logits to probabilities, and getting the results\n",
    "\n",
    "We will now discuss each of these, individually.\n",
    "\n",
    "## Tokenization (Preprocessing; Encoding)\n",
    "\n",
    "Neural networks cannot process textual data. It must be \"Tokenized\".\n",
    "\n",
    "Plainly, the Tokenization process has three steps:\n",
    "1. Converting textual data (sentences, phrases, words) into words, subwords, or symbols, all being **tokens**.\n",
    "2. Converting the tokens into a numeric representations.\n",
    "3. Adding special characters that the model might need to separate sentences.\n",
    "\n",
    "Note that the vocabulary of a model is the corpus it was trained on.\n",
    "\n",
    "Also, there are three main types of Tokenization:\n",
    "1. Word: (worst) Creates huge vocabulary; if vocab is condensed, lots of unknowns; moderate # of tokens to input\n",
    "   - Dog not found in [Dogs], but Dogs is just the plural of dogs... \n",
    "2. Character: (better) (good for ideogram-based languages) less meaningful, but small vocabulary; huge # of tokens to input\n",
    "3. Sub-word: (best) words broken down to make smaller vocabulary\n",
    "   - Dog found in [Dog, s], but [Dog, s] is actually Dogs\n",
    "\n",
    "There are other ways to Tokenize, but this depends on which model you want to use.\n",
    "\n",
    "Each model has it's own preprocessing (tokenization) method. You might ask \"how does this make sense?\" \n",
    "\n",
    "It makes sense because numerically encoding words/tokens and training a model means that if one does not give words the same encoding as they were given in training, then the words effectively aren't the same. Imagine if you read two dictionaries and the definitions of words were completely different!\n",
    "\n",
    "Hugging Face allows us to easily use model's original preprocessing techniques through the `AutoTokenizer` class.\n",
    "\n",
    "*Def'n.* **checkpoint** - The weights used as the parameters for the trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4f6fdd-6e35-4789-b069-2b44550eef24",
   "metadata": {},
   "source": [
    "### Tokenizer Code\n",
    "\n",
    "We can get the tokenizer by explicitly using the BERT tokenizer, or using the AutoTokenizer. Note how `tokenizer` and `tokenizer2` call the same pretrained tokenizer and therefore have the exact same output.\n",
    "\n",
    "Note, **this is the long way to tokenize** so we can show decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1273f194-8d8e-4179-9d69-76a4bc9d4e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'love', 'to', 'learn', 'about', 'LL', '##Ms', '!']\n",
      "['I', 'love', 'to', 'learn', 'about', 'LL', '##Ms', '!']\n",
      "\n",
      "[146, 1567, 1106, 3858, 1164, 12427, 25866, 106]\n",
      "[146, 1567, 1106, 3858, 1164, 12427, 25866, 106]\n",
      "\n",
      "{'input_ids': [101, 146, 1567, 1106, 3858, 1164, 12427, 25866, 106, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 146, 1567, 1106, 3858, 1164, 12427, 25866, 106, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, AutoTokenizer\n",
    "import tensorflow as tf\n",
    "checkpoint = 'bert-base-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(checkpoint)\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = 'I love to learn about LLMs!'\n",
    "\n",
    "# Tokenization\n",
    "tokens1 = tokenizer.tokenize(sequence)\n",
    "tokens2 = tokenizer2.tokenize(sequence)\n",
    "print(tokens1)\n",
    "print(tokens2, end = '\\n\\n')\n",
    "\n",
    "# Encoding\n",
    "ids1 = tokenizer.convert_tokens_to_ids(tokens1)\n",
    "ids2 = tokenizer2.convert_tokens_to_ids(tokens2)\n",
    "print(ids1)\n",
    "print(ids2, end = '\\n\\n')\n",
    "\n",
    "# The output for both tokenizers is the exact same, and for the code above\n",
    "# This is the shortcut code to what was done above\n",
    "print(tokenizer('I love to learn about LLMs!'))\n",
    "print(tokenizer2('I love to learn about LLMs!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fa5f9d-4a4c-4031-ae28-da9734e610ce",
   "metadata": {},
   "source": [
    "The **output for both tokenizers is the exact same; one is just being more explicit.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6be6f8f-d058-407b-876d-be463d82b402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('BertTokenizerFiles\\\\tokenizer_config.json',\n",
       " 'BertTokenizerFiles\\\\special_tokens_map.json',\n",
       " 'BertTokenizerFiles\\\\vocab.txt',\n",
       " 'BertTokenizerFiles\\\\added_tokens.json')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save if you want the Tokenizer's data locally\n",
    "some_file_name = 'BertTokenizerFiles' # could be any name\n",
    "tokenizer.save_pretrained(some_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6a8521-d714-4795-a0bf-95e160a798af",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "At first, I was going to call this section \"Modeling\", then I realized it's not actually modeling because we're using pretrained models; I'll use the Hugging Face section name \"Models\".\n",
    "\n",
    "First, I'll initialize a configured model, but we wont use it because it isn't trained; it's just for completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a1e4a62-aa3f-4d9a-a870-fdc5498f1707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an untrained model with BERT architecture\n",
    "from transformers import BertConfig, TFBertModel\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "config = BertConfig() # build config\n",
    "\n",
    "model = TFBertModel(config) # Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7a7ffdad-c6fa-4584-ae78-ffac3d49a1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.36.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "<transformers.models.bert.modeling_tf_bert.TFBertModel object at 0x000001EDD9565B10>\n"
     ]
    }
   ],
   "source": [
    "print(config)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fa85e2-1041-46e6-b1da-0917490e7cc4",
   "metadata": {},
   "source": [
    "It's good to see what's going on, but we wont use this model because it's untrained. As is taught in the first part of the Hugging Face course: training a huge model can take days and weeks and cost a ton of money.\n",
    "\n",
    "Again, I'll show that the two classes produce the same output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af791a3c-bc38-41bf-8e81-2e9478dd499d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import TFAutoModel, TFBertModel\n",
    "\n",
    "auto_model = TFAutoModel.from_pretrained(checkpoint)\n",
    "bert_model = TFBertModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3924b277-4859-469a-990b-bb96ae232d42",
   "metadata": {},
   "source": [
    "**If you need to save the model, like for after fine-tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e3dc3b-afba-4668-bd9b-0ffefc2e3d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model\n",
    "auto_model.save_pretrained('some_directory_on_pc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5b68464-70ec-4a05-98b8-fd3915ea653a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(2, 12), dtype=int32, numpy=\n",
      "array([[  101, 17662,  2227,  2003,  1037,  3835,  4037,   999,   102,\n",
      "            0,     0,     0],\n",
      "       [  101,  1045,  2293,  2000,  4553,  2055,  6627,  1011,  3141,\n",
      "         7832,   999,   102]])>, 'attention_mask': <tf.Tensor: shape=(2, 12), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])>}\n",
      "{'input_ids': <tf.Tensor: shape=(2, 12), dtype=int32, numpy=\n",
      "array([[  101, 17662,  2227,  2003,  1037,  3835,  4037,   999,   102,\n",
      "            0,     0,     0],\n",
      "       [  101,  1045,  2293,  2000,  4553,  2055,  6627,  1011,  3141,\n",
      "         7832,   999,   102]])>, 'attention_mask': <tf.Tensor: shape=(2, 12), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])>}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "data = ['Hugging Face is a nice website!',\n",
    "        'I love to learn about tech-related topics!']\n",
    "\n",
    "inputs = tokenizer(data, padding = True, truncation = True, return_tensors = 'tf')\n",
    "print(inputs)\n",
    "\n",
    "inputs = tokenizer(data, padding = True,return_tensors = 'tf')\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36f0fa80-1062-4426-a1c2-5bae66756ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\linda\\OneDrive\\Desktop\\Large Language Models\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['pre_classifier.bias', 'classifier.weight', 'classifier.bias', 'pre_classifier.weight']\n",
      "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf4fb0ae-b925-4691-b70e-b6b3f8830725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 12, 768)\n"
     ]
    }
   ],
   "source": [
    "outputs = model(inputs)\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a25030da-d380-4e41-9712-97d7b439a59d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 12, 768), dtype=float32, numpy=\n",
       "array([[[ 0.5692251 , -0.03803831,  0.32966405, ...,  0.6067085 ,\n",
       "          0.98890364, -0.384846  ],\n",
       "        [ 0.7754908 ,  0.09545754,  0.4908302 , ...,  0.76056325,\n",
       "          1.0096803 , -0.30047473],\n",
       "        [ 0.86226773,  0.01781805,  0.4306031 , ...,  0.6162475 ,\n",
       "          0.98571277, -0.25459784],\n",
       "        ...,\n",
       "        [ 0.4978409 , -0.12197125,  0.2099949 , ...,  0.7613839 ,\n",
       "          0.90970665, -0.2837196 ],\n",
       "        [ 0.57905954, -0.04940146,  0.20574325, ...,  0.72581595,\n",
       "          0.9981229 , -0.24939202],\n",
       "        [ 0.4884028 ,  0.07165703,  0.26795864, ...,  0.78447723,\n",
       "          0.8851079 , -0.22291045]],\n",
       "\n",
       "       [[ 0.55294013, -0.16169162,  0.25375643, ...,  0.4124907 ,\n",
       "          0.98647946, -0.506946  ],\n",
       "        [ 1.0206703 ,  0.04772499,  0.03159535, ...,  0.4072253 ,\n",
       "          0.95650345, -0.27522287],\n",
       "        [ 0.8618861 ,  0.18678255,  0.33349323, ...,  0.39239886,\n",
       "          0.86635435, -0.3706951 ],\n",
       "        ...,\n",
       "        [ 0.6109112 ,  0.4583126 ,  0.5045931 , ...,  0.32254872,\n",
       "          0.5442291 , -0.6316707 ],\n",
       "        [ 0.6976254 , -0.01707018,  0.15228714, ...,  0.5006288 ,\n",
       "          0.9809944 , -0.36704078],\n",
       "        [ 1.310964  ,  0.23489735,  0.29237738, ...,  0.8093303 ,\n",
       "          0.3899331 , -0.6570169 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f988706d-e398-490c-9bc2-bd4bc4ebe5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "model2 = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs2 = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6dd0803d-58eb-4dc3-a9d1-fef61fc5c820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFBaseModelOutput(last_hidden_state=<tf.Tensor: shape=(2, 12, 768), dtype=float32, numpy=\n",
       "array([[[ 0.5692251 , -0.03803831,  0.32966405, ...,  0.6067085 ,\n",
       "          0.98890364, -0.384846  ],\n",
       "        [ 0.7754908 ,  0.09545754,  0.4908302 , ...,  0.76056325,\n",
       "          1.0096803 , -0.30047473],\n",
       "        [ 0.86226773,  0.01781805,  0.4306031 , ...,  0.6162475 ,\n",
       "          0.98571277, -0.25459784],\n",
       "        ...,\n",
       "        [ 0.4978409 , -0.12197125,  0.2099949 , ...,  0.7613839 ,\n",
       "          0.90970665, -0.2837196 ],\n",
       "        [ 0.57905954, -0.04940146,  0.20574325, ...,  0.72581595,\n",
       "          0.9981229 , -0.24939202],\n",
       "        [ 0.4884028 ,  0.07165703,  0.26795864, ...,  0.78447723,\n",
       "          0.8851079 , -0.22291045]],\n",
       "\n",
       "       [[ 0.55294013, -0.16169162,  0.25375643, ...,  0.4124907 ,\n",
       "          0.98647946, -0.506946  ],\n",
       "        [ 1.0206703 ,  0.04772499,  0.03159535, ...,  0.4072253 ,\n",
       "          0.95650345, -0.27522287],\n",
       "        [ 0.8618861 ,  0.18678255,  0.33349323, ...,  0.39239886,\n",
       "          0.86635435, -0.3706951 ],\n",
       "        ...,\n",
       "        [ 0.6109112 ,  0.4583126 ,  0.5045931 , ...,  0.32254872,\n",
       "          0.5442291 , -0.6316707 ],\n",
       "        [ 0.6976254 , -0.01707018,  0.15228714, ...,  0.5006288 ,\n",
       "          0.9809944 , -0.36704078],\n",
       "        [ 1.310964  ,  0.23489735,  0.29237738, ...,  0.8093303 ,\n",
       "          0.3899331 , -0.6570169 ]]], dtype=float32)>, hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
