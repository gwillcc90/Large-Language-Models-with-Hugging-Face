{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79aff2a5-fe18-49b2-8b13-ba0e0206544a",
   "metadata": {},
   "source": [
    "# The Hugging Face Pipeline\n",
    "\n",
    "First, why are we even using Hugging Face? Is there something better? What does Hugging Face offer us?\n",
    "\n",
    "Hugging Face strives to be *the* hub for sharing LLMs. Because anyone can share LLMs, and big companies *do* share their LLMs here, we can use models with 10s of billions of parameters with little fuss.\n",
    "\n",
    "Hugging Face advertises:\n",
    "- **It's easy**: \"Downloading, loading, and using LLMs is done in a few lines of code\"\n",
    "  - We see this is true by the by performing a task in two lines of code using `pipeline`\n",
    "- **Flexibility**: Everything is of the class Pytorch and TensorFlow\n",
    "\n",
    "We've used the `pipeline` function.\n",
    "\n",
    "The `pipeline` function is abstracting away the process of:\n",
    "1. **Preprocessing**: Tokenization and Encoding\n",
    "2. **Model Input**: Inputting the preprocessed data into the model and outputting logits\n",
    "3. **Postprocessing**: converting logits to probabilities, and getting the results\n",
    "\n",
    "We will now discuss each of these, individually.\n",
    "\n",
    "## 1. Preprocessing (Tokenization and Encoding)\n",
    "\n",
    "Neural networks cannot process textual data. It must be encoded into numbers.\n",
    "\n",
    "Plainly, the Preprocessing process has three steps:\n",
    "1. **Tokenization**: Converting textual data (sentences, phrases, words) into words, subwords, or symbols, all being **tokens**.\n",
    "2. **Encoding**: Converting the tokens into a numeric representations.\n",
    "3. Adding special characters that the model might need to separate sentences.\n",
    "\n",
    "Note that the vocabulary of a model is the corpus it was trained on.\n",
    "\n",
    "Also, there are three main types of Tokenization:\n",
    "1. Word: (worst) Creates huge vocabulary; if vocab is condensed, lots of unknowns; moderate # of tokens to input\n",
    "   - \"Dog\" has no relation to \"Dogs\", but Dogs is just the plural of dogs... \n",
    "2. Character: (better) (good for ideogram-based languages) less meaningful, but small vocabulary; huge # of tokens to input\n",
    "3. Sub-word: (best) words broken down to make smaller vocabulary\n",
    "   - Dog found in [Dog, s], but [Dog, s] is actually Dogs\n",
    "\n",
    "There are other ways to Tokenize, but this depends on which model you want to use.\n",
    "\n",
    "Each model has it's own preprocessing (tokenization/encoding) method. You might ask \"how does this make sense?\" \n",
    "\n",
    "It makes sense because numerically encoding words/tokens and training a model means that if one does not give words the same encoding as they were given in training, then the words effectively aren't the same. Imagine if you read two dictionaries and the definitions of words were completely different!\n",
    "\n",
    "Hugging Face allows us to easily use model's original preprocessing techniques through the `AutoTokenizer` class.\n",
    "\n",
    "*Def'n.* **checkpoint** - The weights used as the parameters for the trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4f6fdd-6e35-4789-b069-2b44550eef24",
   "metadata": {},
   "source": [
    "### Tokenizer Code\n",
    "\n",
    "We can get the tokenizer by explicitly using the BERT tokenizer, or using the AutoTokenizer. Note how `tokenizer` and `tokenizer2` call the same pretrained tokenizer and therefore have the exact same output.\n",
    "\n",
    "**This is the long way to tokenize** using `transformers` so we can show decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1273f194-8d8e-4179-9d69-76a4bc9d4e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\linda\\OneDrive\\Desktop\\Large Language Models\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\linda\\OneDrive\\Desktop\\Large Language Models\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "['I', 'love', 'to', 'learn', 'about', 'LL', '##Ms', '!']\n",
      "['I', 'love', 'to', 'learn', 'about', 'LL', '##Ms', '!']\n",
      "\n",
      "[146, 1567, 1106, 3858, 1164, 12427, 25866, 106]\n",
      "[146, 1567, 1106, 3858, 1164, 12427, 25866, 106]\n",
      "\n",
      "{'input_ids': [101, 146, 1567, 1106, 3858, 1164, 12427, 25866, 106, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 146, 1567, 1106, 3858, 1164, 12427, 25866, 106, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, AutoTokenizer\n",
    "import tensorflow as tf\n",
    "checkpoint = 'bert-base-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(checkpoint)\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = 'I love to learn about LLMs!'\n",
    "\n",
    "# Tokenization\n",
    "tokens1 = tokenizer.tokenize(sequence)\n",
    "tokens2 = tokenizer2.tokenize(sequence)\n",
    "print(tokens1)\n",
    "print(tokens2, end = '\\n\\n')\n",
    "\n",
    "# Encoding\n",
    "ids1 = tokenizer.convert_tokens_to_ids(tokens1)\n",
    "ids2 = tokenizer2.convert_tokens_to_ids(tokens2)\n",
    "print(ids1)\n",
    "print(ids2, end = '\\n\\n')\n",
    "\n",
    "# The output for both tokenizers is the exact same, and for the code above\n",
    "# This is the shortcut code to what was done above\n",
    "print(tokenizer('I love to learn about LLMs!'))\n",
    "print(tokenizer2('I love to learn about LLMs!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fa5f9d-4a4c-4031-ae28-da9734e610ce",
   "metadata": {},
   "source": [
    "The **output for both tokenizers is the exact same; one is just being more explicit.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6be6f8f-d058-407b-876d-be463d82b402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('BertTokenizerFiles\\\\tokenizer_config.json',\n",
       " 'BertTokenizerFiles\\\\special_tokens_map.json',\n",
       " 'BertTokenizerFiles\\\\vocab.txt',\n",
       " 'BertTokenizerFiles\\\\added_tokens.json')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save if you want the Tokenizer's data locally\n",
    "some_file_name = 'BertTokenizerFiles' # could be any name\n",
    "tokenizer.save_pretrained(some_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6a8521-d714-4795-a0bf-95e160a798af",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "At first, I was going to call this section \"Modeling\", then I realized it's not actually modeling because we're using pretrained models; I'll use the Hugging Face section name \"Models\".\n",
    "\n",
    "First, I'll initialize a configured model, but we wont use it because it isn't trained; it's just for completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a1e4a62-aa3f-4d9a-a870-fdc5498f1707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an untrained model with BERT architecture\n",
    "from transformers import BertConfig, TFBertModel\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "config = BertConfig() # build config\n",
    "\n",
    "model = TFBertModel(config) # Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7a7ffdad-c6fa-4584-ae78-ffac3d49a1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.36.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "<transformers.models.bert.modeling_tf_bert.TFBertModel object at 0x000001EDD9565B10>\n"
     ]
    }
   ],
   "source": [
    "print(config) # Outputs model hyperparameters\n",
    "print(model) # Outputs the class name information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fa85e2-1041-46e6-b1da-0917490e7cc4",
   "metadata": {},
   "source": [
    "It's good to see what's going on, but we wont use this model because it's untrained. As is taught in the first part of the Hugging Face course: training a huge model can take days and weeks and cost a ton of money.\n",
    "\n",
    "Again, I'll show that the two classes produce the same output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "af791a3c-bc38-41bf-8e81-2e9478dd499d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModel, TFBertModel\n",
    "\n",
    "auto_model = TFAutoModel.from_pretrained(checkpoint)\n",
    "bert_model = TFBertModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4fab6fd-2b02-4567-a757-88cb0bc40a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "sequences = [sequence,\n",
    "            'actually, I love to learn about anything tech related!',\n",
    "            'and I love to learn about languages',\n",
    "            'NLP is perfect!']\n",
    "\n",
    "# Just use the tokenizer, it saves having to pad manually, which takes maybe 4-5 more lines\n",
    "inputs = tokenizer(sequences, padding = True, truncation=True, return_tensors = 'tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfae2405-bfbe-4486-927e-f74b1b87473a",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = auto_model(inputs)\n",
    "outputs2 = bert_model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "73cd778a-28e5-4160-b0c0-b74da3ad8448",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# (outputs2[0] == outputs[0]) # Trust me, this outputs True..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cf4fb0ae-b925-4691-b70e-b6b3f8830725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 13, 768)\n"
     ]
    }
   ],
   "source": [
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ae4884-4496-4274-b5a4-c2892ad1d580",
   "metadata": {},
   "source": [
    "**This output was shown for completeness, but to perform a task, we need the proper attention head for that task.** In the next output it's also easier to show the models are outputting the exact same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f988706d-e398-490c-9bc2-bd4bc4ebe5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification, TFBertForSequenceClassification\n",
    "\n",
    "auto_clf = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "bert_clf = TFBertForSequenceClassification.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea6bf004-c5c8-43f1-8dc4-d2f1595443b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"bert-base-cased\",\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.36.2\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 28996\n",
       "}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_clf.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79cb36c8-d563-4542-bac1-705410cf8f50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"bert-base-cased\",\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.36.2\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 28996\n",
       "}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_clf.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6eb4bafa-c91b-4d9c-8331-7e3772d31356",
   "metadata": {},
   "outputs": [],
   "source": [
    "out1 = auto_clf(inputs)\n",
    "out2 = bert_clf(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e85f7686-29c6-4f53-86e6-8ac00ba727bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.48359364 0.51640636]\n",
      " [0.51789117 0.48210883]\n",
      " [0.49984252 0.50015754]\n",
      " [0.5000088  0.49999118]], shape=(4, 2), dtype=float32)\n",
      "\n",
      "tf.Tensor(\n",
      "[[0.6227092  0.37729082]\n",
      " [0.6379381  0.3620619 ]\n",
      " [0.6230812  0.37691882]\n",
      " [0.6356983  0.3643017 ]], shape=(4, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.math.softmax(out1.logits, axis = 1))\n",
    "print()\n",
    "print(tf.math.softmax(out2.logits, axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3924b277-4859-469a-990b-bb96ae232d42",
   "metadata": {},
   "source": [
    "**If you need to save the model, like for after fine-tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e3dc3b-afba-4668-bd9b-0ffefc2e3d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model\n",
    "auto_model.save_pretrained('some_directory_on_pc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
